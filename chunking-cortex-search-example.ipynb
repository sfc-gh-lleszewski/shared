{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "imports",
    "collapsed": false
   },
   "source": "#Import python packages & establish session\nimport pandas as pd\nfrom PyPDF2 import PdfFileReader\nfrom snowflake.snowpark.files import SnowflakeFile\nfrom io import BytesIO\n\nfrom snowflake.snowpark.types import StringType, StructField, StructType, IntegerType\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8420405b-55ba-4deb-9a0f-fe9cb6b6fde0",
   "metadata": {
    "language": "sql",
    "name": "list_files",
    "collapsed": false
   },
   "outputs": [],
   "source": "ls @pdfs",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8d8141a-199f-41fb-bfbe-7f136f5b3fde",
   "metadata": {
    "name": "cell7",
    "collapsed": false
   },
   "source": "Let's define a Python function ```readpdf``` that reads and extracts text from a PDF file. This function is then registered as a UDTF in Snowflake to provide the scalability to process multiple PDFs in parallel across the nodes of a WH "
  },
  {
   "cell_type": "code",
   "id": "ed8a8764-ef9d-4622-bec9-08902d357e69",
   "metadata": {
    "language": "python",
    "name": "cell5",
    "collapsed": false
   },
   "outputs": [],
   "source": "def readpdf(file_path):\n    whole_text = \"\"\n    with SnowflakeFile.open(file_path, 'rb') as file:\n        f = BytesIO(file.readall())\n        pdf_reader = PdfFileReader(f)\n        whole_text = \"\"\n        for page in pdf_reader.pages:\n            whole_text += page.extract_text()\n    return whole_text\n\n#Register the UDF. \nsession.udf.register(\n    func = readpdf\n  , return_type = StringType()\n  , input_types = [StringType()]\n  , is_permanent = True\n  , name = 'readpdf'\n  , replace = True\n  , packages=['snowflake-snowpark-python','pypdf2']\n  , stage_location = 'pdfs'\n)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ef7dd26d-24ba-4668-a0f9-eade892f760c",
   "metadata": {
    "language": "sql",
    "name": "cell6",
    "collapsed": false
   },
   "outputs": [],
   "source": "\nSELECT \n    relative_path, \n    file_url, \n    readpdf(build_scoped_file_url(@pdfs, relative_path)) as raw_text\nfrom directory(@pdfs);",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9ef0bf1-5260-4b95-94dc-d87164343a3e",
   "metadata": {
    "name": "cell9",
    "collapsed": false
   },
   "source": "Let's split the text extracted from our PDFs into chunks (contenxtuallly relevant pieces). Think of it like converting large unstructured text into a knowledge base of answers. Converting, cleaning, and checking large documents can be difficult to get right. We will try 2 approaches: fixed chunking and a sentence based chunking:\n\n- Fixed-size chunking is simpler to implement and computationally efficient, but not respect natural language boundaries, leading to potential loss of semantic context between chunks\n- Sentence based chunking helps maintaining context and meaning. Results in chunks of varying sizes, which might not be ideal for models that require uniform input dimensions. Requires more sophisticated processing\n\n\n\n\n"
  },
  {
   "cell_type": "code",
   "id": "45fc0709-89e0-4307-a260-6901b4bc6a51",
   "metadata": {
    "language": "python",
    "name": "udtf_fixed",
    "collapsed": false
   },
   "outputs": [],
   "source": "import io\nimport re\nimport json\nimport pandas as pd\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2\nimport langchain\n\nclass fixed_text_chunker:\n    \n    def read_pdf(self, file_url: str) -> str:\n        whole_text = \"\"\n        with SnowflakeFile.open(file_url, 'rb') as file:\n            f = BytesIO(file.readall())\n            pdf_reader = PdfFileReader(f)\n            whole_text = \"\"\n            for page in pdf_reader.pages:\n                whole_text += page.extract_text()\n        return whole_text\n\n    def process(self, file_url: str, chunk_size:int, chunk_overlap: int): \n        text_raw=[]\n        text_raw.append(self.read_pdf(file_url)) \n        # Read the PDF and get the combined text and page-specific texts\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            separators = [\"\\n\"], # Define an appropriate separator. New line is good typically!\n            chunk_size = chunk_size, #Adjust this as you see fit\n            chunk_overlap  = chunk_overlap, #This let's text have some form of overlap. Useful for keeping chunks contextual\n            length_function = len,\n            add_start_index = True #Optional but useful if you'd like to feed the chunk before/after\n        )\n        \n    \n        chunks = text_splitter.create_documents(text_raw)\n        df = pd.DataFrame([[d.page_content, d.metadata] for d in chunks], columns=['chunks','meta'])\n        \n        yield from df.itertuples(index=False, name=None)\n\n#Register the UDTF - set the stage location\n\nschema = StructType([\n     StructField(\"chunk\", StringType()),\n     StructField(\"meta\", StringType()),\n ])\n\nsession.udtf.register( \n    handler = fixed_text_chunker,\n    output_schema= schema, \n    input_types = [StringType(), IntegerType(), IntegerType()] , \n    is_permanent = True , \n    name = 'fixed_text_chunker' , \n    replace = True , \n    packages=['pandas','langchain','snowflake-snowpark-python','PyPDF2'], stage_location = 'pdfs' )",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc0cde73-1681-49ac-b683-8c2b319cb728",
   "metadata": {
    "language": "sql",
    "name": "preview_fixed",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from table(fixed_text_chunker(build_scoped_file_url( @pdfs , 'Example data for Snowflake.pdf'), 10000, 1000));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c2f4c693-0e42-48fe-b896-e8c6c895ded0",
   "metadata": {
    "language": "sql",
    "name": "create_fixed_table",
    "collapsed": false
   },
   "outputs": [],
   "source": "--Create the chunked version of the table\nCREATE OR REPLACE TABLE FIXED_CHUNK_TABLE AS\nSELECT \n    relative_path,\n    size,\n    build_scoped_file_url(@pdfs, relative_path) AS file_url,\n    func.chunk as chunk,\n    func.meta as starting_index,\n    'English' AS language\nFROM\ndirectory(@pdfs),\nTABLE(fixed_text_chunker(build_scoped_file_url(@pdfs, relative_path),10000, 1000)) AS func;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e34f3687-f0bd-4da1-8651-ee3a2cef6f84",
   "metadata": {
    "language": "sql",
    "name": "fixed_chunk_size",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT len(chunk) as ChunkSize, ROW_NUMBER() OVER (ORDER BY ChunkSize) AS Chunk FROM FIXED_CHUNK_TABLE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "51f48ee0-aaff-48bb-a080-78e19a5c43bb",
   "metadata": {
    "language": "python",
    "name": "chart_fixed_chunks",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n\nmy_df = fixed_chunk_size.to_pandas()\n# Chart the data\nst.subheader(\"Length of Chunks\")\nst.bar_chart(my_df, x='CHUNK', y='CHUNKSIZE')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40aedeec-74f2-43f0-abe0-ea8cd203dbdf",
   "metadata": {
    "name": "info_sentence_chunker",
    "collapsed": false
   },
   "source": "The ```pdf_sentence_chunker``` class is designed to read a PDF file from Snowflake storage, extract text from each page, split the text into sentences, and then combine these sentences into larger chunks while keeping track of the page numbers. The final output is a series of tuples containing the combined sentence and a JSON string of the page numbers, which are yielded one by one. This can be useful for processing large texts where context needs to be preserved across sentence boundaries."
  },
  {
   "cell_type": "code",
   "id": "da0f7c1f-a11a-4b57-85a3-27118ecb7b90",
   "metadata": {
    "language": "python",
    "name": "chunker_sentences",
    "collapsed": false
   },
   "outputs": [],
   "source": "import io\nimport re\nimport json\nimport pandas as pd\nfrom snowflake.snowpark.files import SnowflakeFile\nimport PyPDF2\nfrom unicodedata import normalize\n\nclass pdf_sentence_chunker:\n\n    def read_pdf(self, file_url: str) -> tuple:\n        # Open the PDF file from Snowflake storage and read its contents into a buffer\n        with SnowflakeFile.open(file_url, 'rb') as f:\n            buffer = io.BytesIO(f.readall())\n            \n        # Initialize the PDF reader with the buffer content\n        reader = PyPDF2.PdfReader(buffer)   \n        text = \"\"\n        page_texts = []\n        \n        # Iterate through each page of the PDF\n        for page_num, page in enumerate(reader.pages):\n            try:\n                # Extract text from the current page\n                extracted_text = page.extract_text()\n                if extracted_text:\n                    # Replace newlines and null characters with spaces\n                    page_text = extracted_text.replace('\\n', ' ').replace('\\0', ' ')\n                    text += page_text\n                    page_texts.append((page_num + 1, page_text))\n            except Exception as e:\n                # Handle any exceptions that occur during text extraction\n                text = \"Unable to Extract\"\n        \n        # Return the combined text and a list of page-specific texts\n        return text, page_texts\n    \n    def combine_sentences(self, sentences, max_length: int, buffer_size=2):\n        combined_sentences = []\n        i = 0\n    \n        # Iterate over the sentences to combine them into larger chunks\n        while i < len(sentences):\n            combined_sentence = ''\n            start = max(0, i - buffer_size)\n            end = i\n            page_numbers = set()\n    \n            # Combine sentences until the max_length is reached\n            while end < len(sentences):\n                if len(combined_sentence) + len(sentences[end]['sentence']) + 1 > max_length:\n                    # Add the current sentence even if it exceeds max_length if combined_sentence is empty\n                    if not combined_sentence:\n                        combined_sentence += sentences[end]['sentence'] + ' '\n                        page_numbers.add(sentences[end]['page'])\n                        end += 1\n                    break\n                combined_sentence += sentences[end]['sentence'] + ' '\n                page_numbers.add(sentences[end]['page'])\n                end += 1\n    \n            # Append the combined sentence and associated page numbers to the result list\n            combined_sentences.append((combined_sentence.strip(), json.dumps(sorted(page_numbers))))\n            i = end\n    \n        # Return the list of combined sentences and their page numbers\n        return combined_sentences\n\n    def process(self, file_url: str, max_length: int):\n        # Read the PDF and get the combined text and page-specific texts\n        text, page_texts = self.read_pdf(file_url)\n        \n        # Decode the text if it is in bytes format\n        if isinstance(text, bytes):\n            text = text.decode('utf-8')\n\n        sentences = []\n        # Split the text of each page into individual sentences and collect them with their page numbers\n        for page_num, page_text in page_texts:\n            single_sentences_list = re.split(r'(?<=[.?!])\\s+', page_text)\n            sentences.extend([{'index': i, 'sentence': sent, 'page': page_num} for i, sent in enumerate(single_sentences_list)])\n\n        # Combine sentences into larger chunks\n        combined_sentences = self.combine_sentences(sentences, max_length)\n\n        # Create a DataFrame from the combined sentences\n        df = pd.DataFrame(combined_sentences, columns=['combined_sentence', 'page_numbers'])\n        \n        # Yield each row of the DataFrame as a tuple\n        yield from df.itertuples(index=False, name=None)\n\n#Register the UDTF - set the stage location\nschema = StructType([\n     StructField(\"sentences\", StringType()),\n     StructField(\"page_number\", StringType()),\n ])\n\nsession.udtf.register( \n    handler = pdf_sentence_chunker,\n    output_schema= schema, \n    input_types = [StringType(), IntegerType()] , \n    is_permanent = True , \n    name = 'pdf_sentence_chunker' , \n    replace = True , \n    packages=['snowflake-snowpark-python','PyPDF2', 'PyCryptodome'], stage_location = 'pdfs' )\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79d3f709-6f09-409a-9db7-65f203901227",
   "metadata": {
    "language": "sql",
    "name": "preview_sentences",
    "collapsed": false
   },
   "outputs": [],
   "source": "select * from table(pdf_sentence_chunker(build_scoped_file_url( @pdfs , 'Example data for Snowflake.pdf'), 10000));",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79c6ada1-2b55-460a-8b6f-ec9da2065d1e",
   "metadata": {
    "language": "sql",
    "name": "create_sentences_table",
    "collapsed": false
   },
   "outputs": [],
   "source": "--Create the chunked version of the table\nCREATE OR REPLACE TABLE SENTENCES_TABLE AS\nSELECT \n    relative_path,\n    size,\n    build_scoped_file_url(@pdfs, relative_path) AS file_url,\n    func.sentences as chunk,\n    func.page_number as page_number,\n    'English' AS language\nFROM\ndirectory(@pdfs),\nTABLE(pdf_sentence_chunker(build_scoped_file_url(@pdfs, relative_path), 10000)) AS func;\n",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5175260-9be4-434a-9564-6c9320aa1a84",
   "metadata": {
    "language": "sql",
    "name": "sentences_chunk_size",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT len(chunk) as ChunkSize, ROW_NUMBER() OVER (ORDER BY ChunkSize) AS Chunk FROM SENTENCES_TABLE",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8e1eef61-5696-4168-abfe-8608d6d54cf3",
   "metadata": {
    "language": "sql",
    "name": "preview_sentences_table",
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM SENTENCES_TABLE LIMIT 5;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4c35760a-3e03-4b11-822b-9288cd8338a0",
   "metadata": {
    "language": "python",
    "name": "preview",
    "collapsed": false
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n\nmy_df = sentences_chunk_size.to_pandas()\n# Chart the data\nst.subheader(\"Length of Chunks\")\nst.bar_chart(my_df, x='CHUNK', y='CHUNKSIZE')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0935240a-46f3-40fe-8cb7-5dd0728a4bc1",
   "metadata": {
    "language": "sql",
    "name": "cortex_search_fixed",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE \nfixed_chunk_search\n    ON chunk\n    ATTRIBUTES language\n    WAREHOUSE = compute_wh\n    TARGET_LAG = '1 hour'\n    AS (\n    SELECT\n        chunk,\n        file_url,\n        relative_path,\n        language\n    FROM FIXED_CHUNK_TABLE\n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3159f184-95f9-4493-a4fe-3c461d885c42",
   "metadata": {
    "language": "sql",
    "name": "cortex_service_sentences",
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE CORTEX SEARCH SERVICE \nsentence_chunk_search\n    ON chunk\n    ATTRIBUTES language\n    WAREHOUSE = compute_wh\n    TARGET_LAG = '1 hour'\n    AS (\n    SELECT\n        chunk,\n        page_number,\n        file_url,\n        relative_path,\n        language\n    FROM SENTENCES_TABLE\n    );",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3393d5b8-195b-486e-b908-ff20f99cd12b",
   "metadata": {
    "language": "python",
    "name": "test_cortex_sentences1",
    "collapsed": false
   },
   "outputs": [],
   "source": "from snowflake.core import Root\n\nroot = Root(session)\n\npdf_service = (root\n  .databases[\"ALLEGRO_HACKATHON\"]\n  .schemas[\"PUBLIC\"]\n  .cortex_search_services[\"sentence_chunk_search\"]\n)\n\nresp = pdf_service.search(\n  query=\"In which table I can find the tenor quantity of credit contract?\",\n  columns=[\"chunk\", \"page_number\"],\n  filter={\"@eq\": {\"language\": \"English\"} },\n  limit=3\n)\nprint(resp.to_json())",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2e9ba2e2-e58c-4ed5-91d1-42066816c36e",
   "metadata": {
    "language": "python",
    "name": "test_cortex_sentences2",
    "collapsed": false
   },
   "outputs": [],
   "source": "\n\nresp = pdf_service.search(\n  query=\"What columns does the BALANCE_CHANGED table contain?\",\n  columns=[\"chunk\", \"page_number\"],\n  filter={\"@eq\": {\"language\": \"English\"} },\n  limit=3\n)\nprint(resp.to_json())",
   "execution_count": null
  }
 ]
}